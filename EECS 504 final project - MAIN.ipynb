{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"EECS 504 final project - MAIN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hu4KFlFWgbXx"},"source":["Install required libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6qwOZFJIB7H","executionInfo":{"status":"ok","timestamp":1618617196132,"user_tz":240,"elapsed":5593,"user":{"displayName":"Spencer Cook","photoUrl":"","userId":"05787087293104816842"}},"outputId":"e9cc7b88-4991-457a-e012-7b005767956c"},"source":["!pip install wandb\n","!pip install -U scikit-learn"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.26)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n","Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.2.0)\n","Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n","Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j-wzwhJ-hyYF"},"source":["Please make sure that you mount your Google Drive so that you can access the datasets of images in the EECS 504 Shared Folder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gpQ0t_Z9mLe","executionInfo":{"status":"ok","timestamp":1618756943308,"user_tz":240,"elapsed":39957,"user":{"displayName":"Spencer Gable-Cook","photoUrl":"","userId":"09809321037722265364"}},"outputId":"fefb13b7-c51a-4b93-fffc-5eb4e96bcbef"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MkS_mz85g_U-"},"source":["Import libraries"]},{"cell_type":"code","metadata":{"id":"c7qfKg0X0XOB"},"source":["import os\n","import sys\n","import torch\n","import torchvision\n","from torchvision import transforms, datasets\n","import numpy as np\n","import sklearn\n","from sklearn.neighbors import KNeighborsClassifier\n","import wandb #Import this to collab\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","import torchvision.utils as vutils\n","import torch.nn as nn\n","import torch.optim as optim\n","#from config import config\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9veTWxceMds"},"source":["Hyperparameter"]},{"cell_type":"code","metadata":{"id":"Zqrf2jDMtRvN"},"source":["# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","batch_size = 128\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","# ngf=256\n","\n","# Size of feature maps in discriminator\n","ndf = 64\n","# ndf=256\n","\n","# Number of training epochs\n","num_epochs = 10\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","\n","# Save per * steps\n","save_interval = 10\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# Number of layers in the loop in discriminator\n","dn = 3\n","# Number of layers in the loop in generator\n","gn = 3\n","\n","# Digit we are training\n","digit = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL-DwntiXoeP"},"source":["hparams = {'networks': {'discriminator': {'in_dim': nc, 'h_dims': ndf, 'num_layers': dn},\n","                       'generator': {'in_dim': nz, 'h_dims': ngf, 'out_dim': nc, 'num_layers': gn}},\n","          'device': 'gpu',\n","          'exp': 'experiment_name',\n","          'batch_size': batch_size,\n","          'optim': 'Adam', \n","          'lr': lr,\n","          'epochs': num_epochs,\n","          'save_interval': save_interval}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5Igp8y0vMoc"},"source":["Connect to GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWsg6FvAvOKP","executionInfo":{"status":"ok","timestamp":1618617197276,"user_tz":240,"elapsed":6714,"user":{"displayName":"Spencer Cook","photoUrl":"","userId":"05787087293104816842"}},"outputId":"bdccc73a-f434-4121-a08a-cc6a98f5ecc3"},"source":["# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using the GPU!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JVgeE-5LnzVc"},"source":["# Load data"]},{"cell_type":"markdown","metadata":{"id":"APrMi3I0n3eP"},"source":["Load SVHN Dataset - run this dataset"]},{"cell_type":"code","metadata":{"id":"AbkvjLCJn9Lq"},"source":["# Import from our Shared Google Drive\n","# Set directories for training and testing datasets\n","root_dir = \"/content/drive/Shareddrives/EECS 504 Shared Drive/Datasets/SVHN Dataset/\"\n","train_dir = root_dir + \"train/\" + str(digit) + \"/\" \n","test_dir = root_dir + \"test/\" + str(digit) + \"/\"\n","\n","# Images are 32 x 32\n","img_size = 32*(dn-)\n","image_size = 32\n","\n","# Create transforms\n","train_transform = transforms.Compose([\n","    transforms.Resize(img_size),\n","    transforms.CenterCrop(img_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","])\n","test_transform = transforms.Compose([\n","    transforms.Resize(img_size),\n","    transforms.CenterCrop(img_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","])\n","\n","# Create datasets\n","train_img = datasets.ImageFolder(train_dir, transform=train_transform)\n","test_img = datasets.ImageFolder(test_dir, transform=test_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SEQN5BwpQZ5x"},"source":["Load Freiburg Groceries dataset"]},{"cell_type":"markdown","metadata":{"id":"Nz2i8vweU0wV"},"source":["Transformation\n"]},{"cell_type":"code","metadata":{"id":"_7EJqlwHQeqQ"},"source":["# Import from our Shared Google Drive\n","# Set directories for training and testing datasets\n","# root_dir = \"/content/drive/Shareddrives/EECS 504 Shared Drive/Datasets/Freiburg Groceries Dataset/\"\n","# train_dir = root_dir + \"train/\"\n","# test_dir = root_dir + \"test/\"\n","\n","# # Images are 256 x 256\n","# img_size = 256\n","# image_size = 256\n","\n","# # Create transforms\n","# train_transform = transforms.Compose([\n","#     transforms.Resize(img_size),\n","#     transforms.CenterCrop(img_size),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","# ])\n","# test_transform = transforms.Compose([\n","#     transforms.Resize(img_size),\n","#     transforms.CenterCrop(img_size),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","# ])\n","\n","# # Create datasets\n","# train_img = datasets.ImageFolder(train_dir, transform=train_transform)\n","# test_img = datasets.ImageFolder(test_dir, transform=test_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwMHYEGatJcw"},"source":["Create the loaders"]},{"cell_type":"code","metadata":{"id":"Y1zGx9AqtLOO"},"source":["# may need to change batch sizes\n","trainloaders = torch.utils.data.DataLoader(train_img, batch_size=batch_size, shuffle=True)\n","testloaders = torch.utils.data.DataLoader(test_img, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J59TyLuNqjIS"},"source":["Explore the dataset (optional)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_N7AaHtdqmUv","executionInfo":{"status":"ok","timestamp":1618617197658,"user_tz":240,"elapsed":7085,"user":{"displayName":"Spencer Cook","photoUrl":"","userId":"05787087293104816842"}},"outputId":"df50133f-3de8-42f1-d6f3-9aca3987e062"},"source":["print(\"train_img type   :\",type(train_img))\n","print(\"train_img length :\",len(train_img))\n","# print(\"test_img length :\",len(test_img))\n","print(\"train_img classes:\",train_img.classes)\n","print(\"train_img[0] type:\",type(train_img[0]))\n","print(\"train_img[0][0] t:\",type(train_img[0][0]))\n","print(\"train_img[0][1] t:\",type(train_img[0][1]))\n","print(\"train_img[0][0] s:\",train_img[0][0].size())\n","print(\"train_img[0][1]  :\",train_img[0][1])\n","\n","n_labels = len(train_img.classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_img type   : <class 'torchvision.datasets.folder.ImageFolder'>\n","train_img length : 4220\n","train_img classes: ['1']\n","train_img[0] type: <class 'tuple'>\n","train_img[0][0] t: <class 'torch.Tensor'>\n","train_img[0][1] t: <class 'int'>\n","train_img[0][0] s: torch.Size([3, 32, 32])\n","train_img[0][1]  : 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1xTBLmxkyyX2"},"source":["Display images"]},{"cell_type":"code","metadata":{"id":"97N1DfJQyzsl"},"source":["def imshowIndividual(data_image, tensor=False):\n","    image = data_image[0]\n","    label = data_image[1]\n","    image = image.numpy().transpose((1, 2, 0))\n","\n","    mean = np.array([0.5, 0.5, 0.5])\n","    std = np.array([0.5, 0.5, 0.5],)\n","    image = std * image + mean\n","    image = np.clip(image, 0, 1)\n","\n","    plt.imshow(image)\n","    plt.show()\n","    \n","    print(train_img.classes[label])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BrqpmUgiRzp"},"source":["def imshowIndividualGAN(data_image, tensor=False):\n","    image = data_image\n","\n","    image = image.numpy().transpose((1, 2, 0))\n","\n","    mean = np.array([0.5, 0.5, 0.5])\n","    std = np.array([0.5, 0.5, 0.5],)\n","    image = std * image + mean\n","    image = np.clip(image, 0, 1)\n","\n","    plt.imshow(image)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOmu8eI6scs6"},"source":["def plotGridImages(loaders, batchSize, title):\n","  # Plot grid\n","  real_batch = next(iter(loaders))\n","  plt.figure(figsize=(8, 16))\n","  plt.axis(\"off\")\n","  plt.title(title)\n","  plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:batch_size], padding=2, normalize=True).cpu(),(1,2,0)))\n","  print(real_batch[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXTWVLZ6iwNh"},"source":["# Build Discriminator and Generator\n","The generator is shown in the image below:\n"]},{"cell_type":"code","metadata":{"id":"33vv9gDuCpoW"},"source":["#Build disciminator and generator\n","manualSeed=999\n","torch.manual_seed(manualSeed)\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    Discriminator. Linear layers, hidden layer activation is RELU, Output activation is Sigmoid\n","    in_dim: a scalar, dimension of input\n","    h_dims: a list of dimensions of hidden layers\n","\n","    ngpu: Number of GPUs available. Use 0 for CPU mode.\n","    \"\"\"\n","\n","    \"\"\"\n","    General layout:\n","    self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","    \"\"\"\n","\n","    # def __init__(self, in_dim, h_dims):\n","    def __init__(self, in_dim, h_dims, num_layers, ngpu = 1): # Added on Apr.12th.2021\n","\n","        nc = in_dim # Number of channels in the training images, integer. For color images this is 3\n","        ndf = h_dims # Size of feature maps in discriminator\n","\n","        #Trying Sequential\n","        super(Discriminator, self).__init__()\n","        #self.ngpu = ngpu\n","        lst=[]\n","\n","        # First layer\n","        lst.append(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n","        lst.append(nn.LeakyReLU(0.2, inplace=True))\n","\n","        # num_layers hidden layers\n","        for i in range(0,num_layers): # 0 to 2\n","            current_input = ndf * 2**i\n","            current_output = ndf * 2**(i+1)\n","            lst.append(nn.Conv2d(current_input, current_output, 4, 2, 1, bias=False))\n","            lst.append(nn.BatchNorm2d(current_output))\n","            lst.append(nn.LeakyReLU(0.2, inplace=True))\n","\n","        # Last layer        \n","        lst.append(nn.Conv2d(current_output, 1, 2, 1, 0, bias=False)) # changed kernel from 4 to 2\n","        lst.append(nn.Sigmoid())\n","\n","        self.main = nn.Sequential(*lst)\n","\n","        # define out_activation\n","        self.out_activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    Generator. Linear layer, hidden layer activation is RELU, Output activation is Tanh\n","    in_dim: a scalar, dimension of input\n","    h_dims: a list of dimensions of hidden layers\n","    out_dim: a scalar, dimensin of output\n","\n","    ngpu: Number of GPUs available. Use 0 for CPU mode.\n","    \"\"\"\n","\n","    \"\"\"\n","    General layout:\n","    self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","    \"\"\"\n","\n","    def __init__(self, in_dim, h_dims, out_dim, num_layers, ngpu = 1):\n","        \n","        super(Generator, self).__init__()\n","        \n","        number_of_hidden_layers = gn # 3 hidden layers\n","        nz = in_dim # Size of z latent vector (i.e. size of generator input)\n","        ngf = h_dims # Size of feature maps in generator\n","        nc = out_dim\n","\n","        # Initial layer\n","        # lst=[torch.nn.ConvTranspose2d(nz, ngf * 2**number_of_hidden_layers, 4, 1, 0, bias=False)]\n","        # For 32 x 32 image\n","        lst=[torch.nn.ConvTranspose2d(nz, ngf * 2**number_of_hidden_layers, 2, 1, 0, bias=False)] # changed kernel from 4 to 2\n","        lst.append(nn.BatchNorm2d(ngf * 2**number_of_hidden_layers))\n","        lst.append(nn.ReLU(inplace=True))\n","       \n","        # num_layers hidden layers\n","        for i in range(num_layers,0,-1):\n","            lst.append(nn.ConvTranspose2d(ngf*2**i, ngf*2**(i-1), 4, 2, 1, bias=False)) #check h_dim length\n","            lst.append(nn.BatchNorm2d(ngf*2**(i-1)))\n","            lst.append(nn.ReLU(inplace=True))\n","\n","        # Final layer\n","        lst.append(nn.ConvTranspose2d(ngf, nc, 4,2,1))\n","        lst.append(nn.Tanh())\n","\n","        self.main = nn.Sequential(*lst)    \n","\n","    def forward(self, x):\n","        return self.main(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wVYyvcex9ds"},"source":["def weights_init(m):\n","  \n","  if type(m) == nn.ConvTranspose2d:\n","    nn.init.normal_(m.weight.data,0.0,0.02)\n","  elif type(m) == nn.Conv2d:\n","    nn.init.normal_(m.weight.data,0.0,0.02)\n","  elif type(m) == nn.BatchNorm2d:\n","    nn.init.normal_(m.weight.data, 1.0, 0.02)\n","    nn.init.constant_(m.bias.data, 0)\n","  #  classname = m.__class__.__name__\n","  #  if classname.find('Conv') != -1:\n","  #       nn.init.normal_(m.weight.data, 0.0, 0.02)\n","  #  elif classname.find('BatchNorm') != -1:\n","  #       nn.init.normal_(m.weight.data, 1.0, 0.02)\n","  #       nn.init.constant_(m.bias.data, 0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKKAV0vVgpxx","executionInfo":{"status":"ok","timestamp":1618617201270,"user_tz":240,"elapsed":10683,"user":{"displayName":"Spencer Cook","photoUrl":"","userId":"05787087293104816842"}},"outputId":"2c6ddb38-5251-4f35-a415-1ce9a5246499"},"source":["## Code for initializing conv, relu and batch norm layers with 0 mean and 0.02 stddev\n","netG = Generator(nz, ndf, nc,gn).to(device)\n","netG.apply(weights_init)\n","netD = Discriminator(nc, ndf, dn).to(device)\n","netD.apply(weights_init)\n","print(netG)\n","print(netD)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Generator(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (13): Tanh()\n","  )\n",")\n","Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(512, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n","    (12): Sigmoid()\n","  )\n","  (out_activation): Sigmoid()\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RxtZRemkbVyk"},"source":["Print out parameters"]},{"cell_type":"code","metadata":{"id":"gz5jnLgKbXBp"},"source":["num_params = sum([item.numel() for item in netG.parameters() if item.requires_grad])\n","print(num_params)\n","num_params = sum([item.numel() for item in netD.parameters() if item.requires_grad])\n","print(num_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXohmn91Bno6"},"source":["#Build the GAN\n","\n","# class GAN:\n","#     \"\"\"\n","#     Generative Adversarial Network. Encapsulates a generator network, a discriminator network,\n","#     functionality for generating with the generator, and losses for training both networks.\n","#     \"\"\"\n","#     def __init__(self, hparams):\n","\n","#         #Discriminator from given hyperparameters\n","#         self.discriminator = Discriminator(in_dim=hparams['networks']['discriminator']['in_dim'], # 3\n","#                                                     #h_dims=hparams['networks']['discriminator']['h_dims'], # 64\n","#                                                     #num_layers=hparams['networks']['discriminator']['num_layers']) # 3\n","        \n","#         #Generator from given hyperparameters\n","#         self.generator = Generator(in_dim=hparams['networks']['generator']['in_dim'], # 100\n","#                                             #h_dims=hparams['networks']['generator']['h_dims'], # 64\n","#                                             #out_dim=hparams['networks']['generator']['out_dim'], # 3\n","#                                             #num_layers=hparams['networks']['generator']['num_layers']) # 3\n","        \n","#         # Apply weights to discriminator and generator\n","#         self.discriminator.apply(weights_init)\n","#         self.generator.apply(weights_init)\n","\n","        \n","\n","#         #Move to gpu if available\n","#         if hparams['device'] == 'gpu':\n","#             self.generator.cuda()\n","#             self.discriminator.cuda()\n","\n","#         self.hparams = hparams\n","\n","#     def generate(self, num_samples):\n","#         z = torch.randn((num_samples, self.hparams['networks']['generator']['in_dim'], 1, 1)) # Second argument is nz, or size of generator input (100)\n","#         if self.hparams['device'] == 'gpu':\n","#             z = z.cuda()\n","#         return self.generator(z)\n","\n","    #Loss for discriminator\n","    #def loss_d(self, x, x_g):\n","        \n","        # real_logits = self.discriminator(x)\n","        # # print('real logits shape: ', real_logits.shape)\n","        # fake_logits = self.discriminator(x_g)\n","        # # print('fake logits shape: ', fake_logits.shape)\n","        # # print('real logits values: ', real_logits)\n","        # # print('fake logits values: ', fake_logits)\n","        # logits = torch.vstack((real_logits, fake_logits)) # get to 256 x 1\n","        # logits = torch.squeeze(logits)\n","        # # print('new logits shape: ', logits.shape)\n","        # targets = torch.vstack((torch.zeros((real_logits.shape[0], 1)),\n","        #                         torch.ones((fake_logits.shape[0], 1))))\n","        # targets = torch.squeeze(targets)\n","        # # print(\"targets: \", targets)\n","        # if self.hparams['device'] == 'gpu':\n","        #     targets = targets.cuda()\n","        # # accuracy = accuracy_score(self.discriminator.out_activation(targets).detach().cpu().numpy().round(),\n","        # #                           self.discriminator.out_activation(logits).detach().cpu().numpy().round())\n","        # # discriminator out activation is Sigmoid\n","        # # print('targets shape: ', targets.shape)\n","        # # print('logits shape: ', logits.shape)\n","        # y_true = self.discriminator.out_activation(targets).detach().cpu().numpy().round()\n","        \n","        # # print('y true shape: ',y_true.shape)\n","        # y_pred = self.discriminator.out_activation(logits).detach().cpu().numpy().round()\n","        # y_pred = logits\n","        # # print('y pred shape: ',y_pred.shape)\n","        # # y_true = real_logits\n","        # # y_pred = fake_logits\n","        # # y_pred = y_pred[:,:]\n","        # # print('y pred shape: ',y_pred.shape)\n","        # accuracy = accuracy_score(y_true, y_pred)\n","        # return torch.nn.BCELoss()(logits, targets), accuracy\n","    \n","    #Loss for generator\n","    #def loss_g(self, x_g):\n","        # fake_logits = self.discriminator(x_g)\n","        # fake_logits = torch.squeeze(fake_logits)\n","        # targets = torch.zeros((fake_logits.shape[0], 1))\n","        # targets = torch.squeeze(targets)\n","        # # print('fake_logits shape: ', fake_logits.shape)\n","        # # print('targets shape: ', targets.shape)\n","        # if self.hparams['device'] == 'gpu':\n","        #     targets = targets.cuda()\n","        # return torch.nn.BCELoss()(fake_logits, targets)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-xC2FbjfqAL"},"source":["img_list = [] # for saving images\n","fixed_noise = torch.randn(128, nz, 1, 1).cuda()\n","criterion = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gU_5LMuBGaaT"},"source":["\n","# def train(hparams):\n","#     # Setup experiment.\n","#     # wandb.init(project='EECS 504 - Final', name=hparams['exp'], config=hparams)\n","\n","#     # Load model.\n","#     model = GAN(hparams)\n","#     # wandb.watch(model.generator)\n","#     # wandb.watch(model.discriminator)\n","\n","#     # Load data.\n","#     # data = load_mnist_data(batch_size=hparams['batch_size'] // 2)\n","#     # we use trainloaders as the training data\n","#     data = trainloaders\n","\n","#     # Optimizers.\n","#     optimizer = getattr(torch.optim, hparams['optim'])\n","#     optimizer_d = optimizer(params=model.discriminator.parameters(), lr=hparams['lr'], betas=(beta1, 0.999))\n","#     optimizer_g = optimizer(params=model.generator.parameters(), lr=hparams['lr'], betas=(beta1, 0.999))\n","\n","#     # Train.\n","#     for epoch in range(hparams['epochs']):\n","#         for batch_idx, (x, _) in enumerate(data):\n","#             # Flatten and migrate to GPU\n","#             # x = torch.flatten(x, start_dim=1)\n","#             if hparams['device'] == 'gpu':\n","#                 x = x.cuda()\n","\n","#             # Train discriminator.\n","#             netG.zero_grad()\n","#             x_g = netG(num_samples=x.shape[0]).detach()\n","#             # print(x.shape)\n","#             # print(x_g.shape)\n","#             # output = model.discriminator(x)\n","#             # fake_output = model.discriminator(x_g)\n","#             output_real=netD(x).view(-1)\n","#             output_fake=netD(x_g).view(-1)        \n","#             label_real=0.\n","#             label_real=torch.full((x.shape[0],), 1., dtype=torch.float).cuda()\n","#             label_fake=torch.full((x_g.shape[0],), 0., dtype=torch.float).cuda()\n","\n","            \n","#             loss_d_real=criterion(output_real,label_real) \n","           \n","#             loss_d_real.backward()\n","#             loss_d_fake=criterion(output_fake,label_fake) \n","#             loss_d_fake.backward()\n","#             loss_d =loss_d_real + loss_d_fake\n","\n","#             optimizer_d.step()\n","            \n","\n","#             # Train with fake \n","\n","#             # Train generator.\n","#             netD.zero_grad()\n","#             output_fake=netD(x_g).view(-1)\n","#             label=torch.full((x_g.shape[0],), 1., dtype=torch.float).cuda()   \n","#             loss_g = criterion(output_fake,label)\n","           \n","#             loss_g.backward()\n","#             optimizer_g.step()\n","\n","#             # Logging.\n","#             print(\"epoch: {}/{} | batch: {}/{} | loss_g: {} | loss_d: {} %\".format(\n","#                 epoch, hparams['epochs'], batch_idx, len(data), loss_g, loss_d )) #, acc_d * 100\n","#             # wandb.log({\n","#             #     \"loss_g\": loss_g,\n","#             #     \"loss_d\": loss_d,\n","#             #     \"acc_d\": acc_d,\n","#             # })\n","\n","#             # Save generated images\n","#             with torch.no_grad():\n","#               fake = model.generator(fixed_noise).detach().cpu()\n","#               print(fake.shape)\n","#               if batch_idx % 10 == 0:\n","#                 imshowIndividualGAN(fake[1,:,:,:])\n","#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","\n","#         # Checkpointing and saving samples.\n","#             # if epoch % hparams['save_interval'] == 0:\n","#             # if batch_idx == 10:\n","#             #     with torch.no_grad():\n","#             #         examples = model.generate(num_samples=64)\n","#             #     examples = (examples.reshape(-1, 28, 28) + 1) * 255 / 2\n","#             #     examples = np.round(examples.detach().cpu().numpy())[:64]\n","#             #     fig = plt.figure(figsize=(64, 64))\n","#             #     grid = ImageGrid(fig, 111, nrows_ncols=(8, 8), axes_pad=1)\n","#             #     for i, (ax, im) in enumerate(zip(grid, examples)):\n","#             #         ax.imshow(np.reshape(im, (28, 28)), cmap=plt.get_cmap('gray'))\n","#             #     break\n","#             #     epoch = 6\n","#                 # wandb.log({'Generated samples': plt})\n","#     torch.save(model.generator.state_dict(),'/content/saved_generator_'  + str(sub) +'.pt' ) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1atMc-jaKTa_"},"source":["# def main():\n","#     # train(config) #Need to define config \n","#     train(hparams)\n","\n","\n","# if __name__ == '__main__':\n","#     main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"id":"H9iCEXtPyyDf","executionInfo":{"status":"error","timestamp":1618617652030,"user_tz":240,"elapsed":356697,"user":{"displayName":"Spencer Cook","photoUrl":"","userId":"05787087293104816842"}},"outputId":"0a307596-538d-4b49-8fd0-e51c9663ba3d"},"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","img_list = []\n","G_losses = []\n","D_losses = []\n","iters = 0\n","real_label = 1.\n","fake_label = 0.\n","# model = GAN(hparams)\n","netG = Generator(nz, ndf, nc,gn).to(device)\n","netG.apply(weights_init)\n","netD = Discriminator(nc, ndf, dn).to(device)\n","netD.apply(weights_init)\n","optimizer = getattr(torch.optim, hparams['optim'])\n","optimizer_d = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","\n","#optimizer(params=model.discriminator.parameters(), lr=hparams['lr'], betas=(beta1, 0.999))\n","optimizer_g = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))#optimizer(params=model.generator.parameters(), lr=hparams['lr'], betas=(beta1, 0.999))\n","\n","dataloader = trainloaders\n","# model = GAN(hparams)\n","\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(dataloader, 0):\n","        \n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        ## Train with all-real batch\n","        netD.zero_grad()\n","        # Format batch\n","        real_cpu = data[0].to(device) # x\n","        b_size = real_cpu.size(0) # x.shape[0]\n","        label = torch.full((b_size,), real_label, dtype=torch.float).cuda()\n","        # Forward pass real batch through D\n","        output = netD(real_cpu).view(-1) # output_real\n","        # Calculate loss on all-real batch\n","        errD_real = criterion(output, label) # loss_d_real\n","        # Calculate gradients for D in backward pass\n","        errD_real.backward()\n","        D_x = output.mean().item()\n","\n","        ## Train with all-fake batch\n","        # Generate batch of latent vectors\n","        noise = torch.randn(b_size, nz, 1, 1).cuda()\n","        # Generate fake image batch with G\n","        fake = netG(noise)\n","        label.fill_(fake_label)\n","        # Classify all fake batch with D\n","        output = netD(fake.detach()).view(-1)\n","        # Calculate D's loss on the all-fake batch\n","        errD_fake = criterion(output, label)\n","        # Calculate the gradients for this batch\n","        errD_fake.backward()\n","        D_G_z1 = output.mean().item()\n","        # Add the gradients from the all-real and all-fake batches\n","        errD = errD_real + errD_fake\n","        # Update D\n","        optimizer_d.step()\n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        netG.zero_grad()\n","        label.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass of all-fake batch through D\n","        output = netD(fake).view(-1)\n","        # Calculate G's loss based on this output\n","        errG = criterion(output, label) # real labels\n","        # Calculate gradients for G\n","        errG.backward()\n","        D_G_z2 = output.mean().item()\n","        # Update G\n","        optimizer_g.step()\n","        \n","        # Output training stats\n","        if i % 50 == 0:\n","            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (epoch, num_epochs, i, len(dataloader),\n","                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","        \n","        # Save Losses for plotting later\n","        G_losses.append(errG.item())\n","        D_losses.append(errD.item())\n","        \n","        # Check how the generator is doing by saving G's output on fixed_noise\n","        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n","            with torch.no_grad():\n","                fake = netG(fixed_noise).detach().cpu()\n","            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","            \n","        iters += 1\n","\n","# save last image\n","torch.save(img_list[-1], str(digit)+ '.pt')\n","# save generator\n","torch.save(netG.state_dict(),'saved_generator_'  + str(digit) +'.pt' ) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting Training Loop...\n","[0/10][0/33]\tLoss_D: 1.5655\tLoss_G: 2.8254\tD(x): 0.3955\tD(G(z)): 0.4295 / 0.0664\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-e875c1838804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# For each batch in the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[1;32m    177\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Vq9_pQimmQqT"},"source":["## This part of the code will save 128 Image grid into \n","## individual images under the 'Fake Images' folder in shared drive\n","## NOTE: Make suree to change the folder digit name before running\n","for i in range(len(generated_image)):\n","  single_image = generated_image[i,:,:,:]\n","  torchvision.utils.save_image(single_image,\"/content/drive/Shareddrives/EECS 504 Shared Drive/Fake Images/5/\"+str(i)+\".jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTvvHo66vDwG"},"source":["print(len(img_list))\n","print(img_list[11].shape)\n","imshowIndividualGAN(img_list[99])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xyrx6SAYvpWq"},"source":["fig = plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHCMV6kVgirf"},"source":["Function to show images - called \"plotGridImages\" defined above\n"]},{"cell_type":"markdown","metadata":{"id":"GmxgXwgOBBQ2"},"source":["# Classification (K Nearest Neighbors)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_-BhkahOYZy-"},"source":["Be sure to run everything before and including \"Loading Data\" part"]},{"cell_type":"markdown","metadata":{"id":"8Gv4Dt3YveG1"},"source":["Image processing"]},{"cell_type":"code","metadata":{"id":"B-6D0CAzvYv8"},"source":["def knn_image_proc(train_img):\n","  '''\n","  Image flattening and normalization for kNN\n","  Input:\n","    train_img: images and labels\n","  Output:\n","    x_train: processed images (flattened to be vectors and normalized)\n","    y: labels\n","  '''\n","  n_train = len(train_img)\n","\n","  x_train_raw = []\n","\n","  y = []\n","\n","  for i in range(n_train):\n","    # x_i = train_img[i][0].reshape(-1)\n","    x_i = np.array(train_img[i][0].reshape(-1))\n","    x_train_raw.append(x_i)\n","    y.append(train_img[i][1])\n","\n","  # Normalize to zero mean so covariance is just matrix multiplication\n","  mean = np.mean(x_train_raw, axis=0, keepdims=True)\n","  x_train = x_train_raw - mean \n","\n","  return x_train, y "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trZusXQLvjVz"},"source":["Reduction"]},{"cell_type":"code","metadata":{"id":"WKVEvZZCtjmm"},"source":["def compute_basis(data, n=300):\n","  \"\"\" \n","  Computing basis for image reduction\n","  Inputs:\n","      data: multi-dimensional array of size number of images * number of pixels. Each row is a flatten image.\n","      n: number of modes to be kept\n","  Output:\n","      eigenvectors: basis\n","  \"\"\"\n","  \n","  eigenvectors = None\n","\n","  sig = np.matmul(np.transpose(data),data)\n","  _, eigenvectors = np.linalg.eig(sig)\n","  eigenvectors = eigenvectors[:, range(n)]\n","\n","  return eigenvectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Id3c1RB2vp3n"},"source":["kNN"]},{"cell_type":"code","metadata":{"id":"KQvR_PCLqg-d"},"source":["# # Self defined kNN function\n","# def knn_predict(eig_train_x, eig_test_x, y_train, y_test, k=1):\n","\n","#   \"\"\"Implement the KNN algorithm. The output should be a vector containing your predictions\"\"\"\n","\n","#   predictions = np.zeros_like(y_test)\n","    \n","#   N, D = eig_train_x.shape\n","#   for i in range(len(y_test)):\n","#     d = np.ones((N,))\n","#     for j in range(N):\n","#       dx = eig_train_x[j,:] - eig_test_x[i,:]\n","#       d[j] = np.linalg.norm(dx)\n","\n","#     x_in = np.argsort(d)\n","#     neb_in = x_in[:k]\n","#     neb_y = y_train[neb_in]\n","\n","#     counts = np.bincount(neb_y)\n","#     predictions[i] = np.argmax(counts)\n","\n","#   return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPc9UwJgSK4e"},"source":["Implementations"]},{"cell_type":"code","metadata":{"id":"X2lAn79IPdUz"},"source":["k = 5 # k nearest neighbors\n","if_POD = True # Do we do POD?\n","\n","# Image flattening and normalization\n","x_train, y = knn_image_proc(train_img)\n","\n","# Image reduction\n","if if_POD:\n","  eigenvectors = compute_basis(x_train, n=300)\n","  x_input = np.matmul(x_train,eigenvectors)\n","else:\n","  x_input = x_train\n","\n","neigh = KNeighborsClassifier(n_neighbors = k)\n","\n","neigh.fit(x_input, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SdM5HmoB8Bg"},"source":["neigh = neighbors.KNeighborsClassifier(n_neighbors = k)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WB5xjHiZvs3v"},"source":["Visualization"]},{"cell_type":"code","metadata":{"id":"woVpaeimvuqU"},"source":["i_plot = 0\n","imshowIndividual(train_img[0])\n","print()"],"execution_count":null,"outputs":[]}]}